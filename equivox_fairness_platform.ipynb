{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equivox Fairness Platform\n",
    "\n",
    "An AI-driven fairness platform for bias-free evaluation of Urdu audio clips in hiring and workplace assessment.\n",
    "\n",
    "This notebook processes 3-8 second Urdu audio clips to evaluate candidates based solely on clarity of thought and problem-solving ability, removing bias from irrelevant human markers like accent, gender, and race."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies\n",
    "\n",
    "First, let's install and import all required libraries for audio processing, machine learning, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "!pip install librosa>=0.9.0\n",
    "!pip install soundfile>=0.10.0\n",
    "!pip install scipy>=1.7.0\n",
    "!pip install transformers>=4.20.0\n",
    "!pip install torch>=1.12.0\n",
    "!pip install scikit-learn>=1.1.0\n",
    "!pip install matplotlib>=3.5.0\n",
    "!pip install seaborn>=0.11.0\n",
    "!pip install umap-learn>=0.5.0\n",
    "!pip install plotly>=5.0.0\n",
    "!pip install ipywidgets>=7.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core audio processing imports\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import scipy.signal\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine learning imports\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import umap\n",
    "\n",
    "# Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Notebook interface imports\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Audio, HTML\n",
    "import warnings\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "# Configure warnings and display settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display system information\n",
    "import platform\n",
    "import psutil\n",
    "\n",
    "print(\"=== System Information ===\")\n",
    "print(f\"Platform: {platform.system()} {platform.release()}\")\n",
    "print(f\"Python Version: {platform.python_version()}\")\n",
    "print(f\"CPU Cores: {psutil.cpu_count()}\")\n",
    "print(f\"RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "print(f\"Available RAM: {psutil.virtual_memory().available / (1024**3):.1f} GB\")\n",
    "\n",
    "print(\"\\n=== Library Versions ===\")\n",
    "print(f\"LibROSA: {librosa.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Scikit-learn: {sklearn.__version__}\")\n",
    "print(f\"Matplotlib: {matplotlib.__version__}\")\n",
    "print(f\"Seaborn: {sns.__version__}\")\n",
    "\n",
    "# Check for CUDA availability\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nüöÄ CUDA Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB\")\n",
    "else:\n",
    "    print(\"\\nüíª Using CPU for processing\")\n",
    "\n",
    "print(\"\\n‚úÖ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic audio processing capabilities\n",
    "print(\"=== Testing Audio Processing Capabilities ===\")\n",
    "\n",
    "# Test librosa functionality\n",
    "try:\n",
    "    # Generate a test sine wave\n",
    "    duration = 3.0  # seconds\n",
    "    sr = 16000  # sample rate\n",
    "    frequency = 440  # A4 note\n",
    "    \n",
    "    t = np.linspace(0, duration, int(sr * duration), False)\n",
    "    test_audio = np.sin(2 * np.pi * frequency * t)\n",
    "    \n",
    "    # Test MFCC extraction\n",
    "    mfccs = librosa.feature.mfcc(y=test_audio, sr=sr, n_mfcc=13)\n",
    "    print(f\"‚úÖ MFCC extraction: {mfccs.shape}\")\n",
    "    \n",
    "    # Test spectral features\n",
    "    spectral_centroids = librosa.feature.spectral_centroid(y=test_audio, sr=sr)\n",
    "    print(f\"‚úÖ Spectral centroid: {spectral_centroids.shape}\")\n",
    "    \n",
    "    # Test chroma features\n",
    "    chroma = librosa.feature.chroma_stft(y=test_audio, sr=sr)\n",
    "    print(f\"‚úÖ Chroma features: {chroma.shape}\")\n",
    "    \n",
    "    print(\"‚úÖ LibROSA audio processing test successful!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå LibROSA test failed: {e}\")\n",
    "\n",
    "# Test transformers model loading (without actually downloading)\n",
    "try:\n",
    "    from transformers import Wav2Vec2Processor\n",
    "    print(\"‚úÖ Transformers library ready for wav2vec2 model\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Transformers test failed: {e}\")\n",
    "\n",
    "print(\"\\nüéâ Basic functionality tests complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Audio Preprocessing Module\n",
    "\n",
    "This module handles loading, normalizing, and validating audio files for consistent processing across different formats and sample rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data structures for audio processing\n",
    "@dataclass\n",
    "class AudioSample:\n",
    "    \"\"\"Data structure for audio samples\"\"\"\n",
    "    file_path: str\n",
    "    audio_data: np.ndarray\n",
    "    sample_rate: int\n",
    "    duration: float\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate audio sample after initialization\"\"\"\n",
    "        if self.duration < 3.0 or self.duration > 8.0:\n",
    "            raise ValueError(f\"Audio duration {self.duration:.2f}s is outside valid range (3-8 seconds)\")\n",
    "        if self.sample_rate != 16000:\n",
    "            print(f\"Warning: Sample rate {self.sample_rate} Hz will be normalized to 16000 Hz\")\n",
    "\n",
    "class AudioPreprocessor:\n",
    "    \"\"\"Audio preprocessing module for loading, normalizing, and validating audio files\"\"\"\n",
    "    \n",
    "    def __init__(self, target_sr: int = 16000, min_duration: float = 3.0, max_duration: float = 8.0):\n",
    "        \"\"\"\n",
    "        Initialize audio preprocessor\n",
    "        \n",
    "        Args:\n",
    "            target_sr: Target sample rate for normalization (default: 16000 Hz)\n",
    "            min_duration: Minimum allowed audio duration in seconds\n",
    "            max_duration: Maximum allowed audio duration in seconds\n",
    "        \"\"\"\n",
    "        self.target_sr = target_sr\n",
    "        self.min_duration = min_duration\n",
    "        self.max_duration = max_duration\n",
    "        self.supported_formats = ['.wav', '.mp3', '.m4a', '.flac', '.ogg']\n",
    "        \n",
    "    def load_audio(self, file_path: str) -> Tuple[np.ndarray, int]:\n",
    "        \"\"\"\n",
    "        Load audio file using librosa with format detection\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to audio file\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (audio_data, original_sample_rate)\n",
    "            \n",
    "        Raises:\n",
    "            FileNotFoundError: If file doesn't exist\n",
    "            ValueError: If file format is not supported\n",
    "            RuntimeError: If audio loading fails\n",
    "        \"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"Audio file not found: {file_path}\")\n",
    "            \n",
    "        file_ext = Path(file_path).suffix.lower()\n",
    "        if file_ext not in self.supported_formats:\n",
    "            raise ValueError(f\"Unsupported audio format: {file_ext}. Supported formats: {self.supported_formats}\")\n",
    "        \n",
    "        try:\n",
    "            # Load audio with librosa (automatically handles various formats)\n",
    "            audio_data, sample_rate = librosa.load(file_path, sr=None, mono=True)\n",
    "            \n",
    "            if len(audio_data) == 0:\n",
    "                raise RuntimeError(\"Loaded audio file is empty\")\n",
    "                \n",
    "            return audio_data, sample_rate\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load audio file {file_path}: {str(e)}\")\n",
    "    \n",
    "    def normalize_sample_rate(self, audio_data: np.ndarray, original_sr: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Normalize audio to target sample rate\n",
    "        \n",
    "        Args:\n",
    "            audio_data: Input audio data\n",
    "            original_sr: Original sample rate\n",
    "            \n",
    "        Returns:\n",
    "            Resampled audio data\n",
    "        \"\"\"\n",
    "        if original_sr == self.target_sr:\n",
    "            return audio_data\n",
    "            \n",
    "        # Use librosa's high-quality resampling\n",
    "        resampled_audio = librosa.resample(audio_data, orig_sr=original_sr, target_sr=self.target_sr)\n",
    "        return resampled_audio\n",
    "    \n",
    "    def validate_duration(self, audio_data: np.ndarray, sample_rate: int) -> float:\n",
    "        \"\"\"\n",
    "        Validate audio duration against constraints\n",
    "        \n",
    "        Args:\n",
    "            audio_data: Audio data array\n",
    "            sample_rate: Sample rate of audio\n",
    "            \n",
    "        Returns:\n",
    "            Duration in seconds\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If duration is outside valid range\n",
    "        \"\"\"\n",
    "        duration = len(audio_data) / sample_rate\n",
    "        \n",
    "        if duration < self.min_duration:\n",
    "            raise ValueError(f\"Audio duration {duration:.2f}s is too short (minimum: {self.min_duration}s)\")\n",
    "        \n",
    "        if duration > self.max_duration:\n",
    "            raise ValueError(f\"Audio duration {duration:.2f}s is too long (maximum: {self.max_duration}s)\")\n",
    "            \n",
    "        return duration\n",
    "    \n",
    "    def apply_noise_reduction(self, audio_data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply basic noise reduction using spectral gating\n",
    "        \n",
    "        Args:\n",
    "            audio_data: Input audio data\n",
    "            \n",
    "        Returns:\n",
    "            Noise-reduced audio data\n",
    "        \"\"\"\n",
    "        # Simple noise reduction using spectral subtraction\n",
    "        # Estimate noise from first 0.5 seconds (assuming it contains background noise)\n",
    "        noise_sample_length = min(int(0.5 * self.target_sr), len(audio_data) // 4)\n",
    "        \n",
    "        if noise_sample_length > 0:\n",
    "            noise_spectrum = np.abs(np.fft.fft(audio_data[:noise_sample_length]))\n",
    "            noise_power = np.mean(noise_spectrum ** 2)\n",
    "            \n",
    "            # Apply gentle high-pass filter to reduce low-frequency noise\n",
    "            if noise_power > 0:\n",
    "                sos = scipy.signal.butter(4, 80, btype='high', fs=self.target_sr, output='sos')\n",
    "                filtered_audio = scipy.signal.sosfilt(sos, audio_data)\n",
    "                return filtered_audio\n",
    "        \n",
    "        return audio_data\n",
    "    \n",
    "    def preprocess_audio(self, file_path: str, apply_noise_reduction: bool = True) -> AudioSample:\n",
    "        \"\"\"\n",
    "        Complete audio preprocessing pipeline\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to audio file\n",
    "            apply_noise_reduction: Whether to apply noise reduction\n",
    "            \n",
    "        Returns:\n",
    "            AudioSample object with processed audio\n",
    "        \"\"\"\n",
    "        # Load audio file\n",
    "        audio_data, original_sr = self.load_audio(file_path)\n",
    "        \n",
    "        # Normalize sample rate\n",
    "        normalized_audio = self.normalize_sample_rate(audio_data, original_sr)\n",
    "        \n",
    "        # Validate duration\n",
    "        duration = self.validate_duration(normalized_audio, self.target_sr)\n",
    "        \n",
    "        # Apply noise reduction if requested\n",
    "        if apply_noise_reduction:\n",
    "            processed_audio = self.apply_noise_reduction(normalized_audio)\n",
    "        else:\n",
    "            processed_audio = normalized_audio\n",
    "        \n",
    "        # Normalize amplitude to [-1, 1] range\n",
    "        max_amplitude = np.max(np.abs(processed_audio))\n",
    "        if max_amplitude > 0:\n",
    "            processed_audio = processed_audio / max_amplitude\n",
    "        \n",
    "        return AudioSample(\n",
    "            file_path=file_path,\n",
    "            audio_data=processed_audio,\n",
    "            sample_rate=self.target_sr,\n",
    "            duration=duration\n",
    "        )\n",
    "\n",
    "# Initialize the audio preprocessor\n",
    "audio_preprocessor = AudioPreprocessor()\n",
    "print(\"‚úÖ Audio preprocessing module initialized!\")\n",
    "print(f\"Target sample rate: {audio_preprocessor.target_sr} Hz\")\n",
    "print(f\"Valid duration range: {audio_preprocessor.min_duration}-{audio_preprocessor.max_duration} seconds\")\n",
    "print(f\"Supported formats: {audio_preprocessor.supported_formats}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio visualization utilities\n",
    "def plot_waveform(audio_sample: AudioSample, title: str = None, figsize: Tuple[int, int] = (12, 4)):\n",
    "    \"\"\"\n",
    "    Plot audio waveform with time axis\n",
    "    \n",
    "    Args:\n",
    "        audio_sample: AudioSample object to plot\n",
    "        title: Optional title for the plot\n",
    "        figsize: Figure size tuple\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Create time axis\n",
    "    time_axis = np.linspace(0, audio_sample.duration, len(audio_sample.audio_data))\n",
    "    \n",
    "    plt.plot(time_axis, audio_sample.audio_data, color='steelblue', linewidth=0.8)\n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    else:\n",
    "        plt.title(f'Waveform - {Path(audio_sample.file_path).name} ({audio_sample.duration:.2f}s)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_spectrogram(audio_sample: AudioSample, title: str = None, figsize: Tuple[int, int] = (12, 6)):\n",
    "    \"\"\"\n",
    "    Plot audio spectrogram\n",
    "    \n",
    "    Args:\n",
    "        audio_sample: AudioSample object to plot\n",
    "        title: Optional title for the plot\n",
    "        figsize: Figure size tuple\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Compute spectrogram\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio_sample.audio_data)), ref=np.max)\n",
    "    \n",
    "    librosa.display.specshow(D, sr=audio_sample.sample_rate, x_axis='time', y_axis='hz')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    else:\n",
    "        plt.title(f'Spectrogram - {Path(audio_sample.file_path).name}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def display_audio_info(audio_sample: AudioSample):\n",
    "    \"\"\"\n",
    "    Display comprehensive information about an audio sample\n",
    "    \n",
    "    Args:\n",
    "        audio_sample: AudioSample object to analyze\n",
    "    \"\"\"\n",
    "    print(f\"=== Audio Information ===\")\n",
    "    print(f\"File: {Path(audio_sample.file_path).name}\")\n",
    "    print(f\"Duration: {audio_sample.duration:.3f} seconds\")\n",
    "    print(f\"Sample Rate: {audio_sample.sample_rate} Hz\")\n",
    "    print(f\"Samples: {len(audio_sample.audio_data):,}\")\n",
    "    print(f\"Amplitude Range: [{np.min(audio_sample.audio_data):.4f}, {np.max(audio_sample.audio_data):.4f}]\")\n",
    "    print(f\"RMS Energy: {np.sqrt(np.mean(audio_sample.audio_data**2)):.4f}\")\n",
    "    \n",
    "    # Zero crossing rate\n",
    "    zcr = librosa.feature.zero_crossing_rate(audio_sample.audio_data)[0]\n",
    "    print(f\"Zero Crossing Rate: {np.mean(zcr):.4f}\")\n",
    "    \n",
    "    # Spectral centroid\n",
    "    spectral_centroids = librosa.feature.spectral_centroid(y=audio_sample.audio_data, sr=audio_sample.sample_rate)[0]\n",
    "    print(f\"Spectral Centroid: {np.mean(spectral_centroids):.2f} Hz\")\n",
    "\n",
    "print(\"‚úÖ Audio visualization utilities loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test audio preprocessing with synthetic audio\n",
    "print(\"=== Testing Audio Preprocessing Module ===\")\n",
    "\n",
    "# Create test audio files with different characteristics\n",
    "test_dir = Path(\"test_audio\")\n",
    "test_dir.mkdir(exist_ok=True)\n",
    "\n",
    "def create_test_audio(filename: str, duration: float, frequency: float = 440, sample_rate: int = 22050):\n",
    "    \"\"\"Create synthetic test audio file\"\"\"\n",
    "    t = np.linspace(0, duration, int(sample_rate * duration), False)\n",
    "    # Create a more complex waveform with harmonics\n",
    "    audio = (np.sin(2 * np.pi * frequency * t) + \n",
    "             0.3 * np.sin(2 * np.pi * frequency * 2 * t) + \n",
    "             0.1 * np.sin(2 * np.pi * frequency * 3 * t))\n",
    "    \n",
    "    # Add some noise for realism\n",
    "    noise = np.random.normal(0, 0.05, len(audio))\n",
    "    audio = audio + noise\n",
    "    \n",
    "    # Normalize\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "    \n",
    "    filepath = test_dir / filename\n",
    "    sf.write(filepath, audio, sample_rate)\n",
    "    return str(filepath)\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    {\"name\": \"valid_audio_5s.wav\", \"duration\": 5.0, \"frequency\": 440, \"sr\": 16000, \"should_pass\": True},\n",
    "    {\"name\": \"high_sr_audio.wav\", \"duration\": 4.0, \"frequency\": 880, \"sr\": 44100, \"should_pass\": True},\n",
    "    {\"name\": \"short_audio.wav\", \"duration\": 2.0, \"frequency\": 220, \"sr\": 16000, \"should_pass\": False},\n",
    "    {\"name\": \"long_audio.wav\", \"duration\": 10.0, \"frequency\": 330, \"sr\": 8000, \"should_pass\": False},\n",
    "]\n",
    "\n",
    "print(\"Creating test audio files...\")\n",
    "for test_case in test_cases:\n",
    "    filepath = create_test_audio(\n",
    "        test_case[\"name\"], \n",
    "        test_case[\"duration\"], \n",
    "        test_case[\"frequency\"], \n",
    "        test_case[\"sr\"]\n",
    "    )\n",
    "    print(f\"‚úÖ Created: {test_case['name']} ({test_case['duration']}s, {test_case['sr']} Hz)\")\n",
    "\n",
    "print(\"\\n=== Testing Preprocessing Pipeline ===\")\n",
    "\n",
    "for test_case in test_cases:\n",
    "    filepath = test_dir / test_case[\"name\"]\n",
    "    print(f\"\\nTesting: {test_case['name']}\")\n",
    "    \n",
    "    try:\n",
    "        # Test preprocessing\n",
    "        audio_sample = audio_preprocessor.preprocess_audio(str(filepath))\n",
    "        \n",
    "        if test_case[\"should_pass\"]:\n",
    "            print(f\"‚úÖ Successfully processed {test_case['name']}\")\n",
    "            display_audio_info(audio_sample)\n",
    "            \n",
    "            # Verify normalization\n",
    "            assert audio_sample.sample_rate == 16000, f\"Sample rate not normalized: {audio_sample.sample_rate}\"\n",
    "            assert 3.0 <= audio_sample.duration <= 8.0, f\"Duration out of range: {audio_sample.duration}\"\n",
    "            assert np.max(np.abs(audio_sample.audio_data)) <= 1.0, \"Audio not normalized to [-1, 1]\"\n",
    "            \n",
    "            print(\"‚úÖ All validations passed\")\n",
    "        else:\n",
    "            print(f\"‚ùå Expected failure but processing succeeded for {test_case['name']}\")\n",
    "            \n",
    "    except (ValueError, RuntimeError) as e:\n",
    "        if not test_case[\"should_pass\"]:\n",
    "            print(f\"‚úÖ Expected failure: {e}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Unexpected failure: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Unexpected error: {e}\")\n",
    "\n",
    "print(\"\\nüéâ Audio preprocessing module testing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate waveform visualization with test audio\n",
    "print(\"=== Audio Visualization Demo ===\")\n",
    "\n",
    "# Load and visualize a valid test audio file\n",
    "valid_test_file = test_dir / \"valid_audio_5s.wav\"\n",
    "\n",
    "if valid_test_file.exists():\n",
    "    try:\n",
    "        # Process the audio\n",
    "        audio_sample = audio_preprocessor.preprocess_audio(str(valid_test_file))\n",
    "        \n",
    "        print(f\"Visualizing: {valid_test_file.name}\")\n",
    "        \n",
    "        # Display comprehensive info\n",
    "        display_audio_info(audio_sample)\n",
    "        \n",
    "        # Plot waveform\n",
    "        plot_waveform(audio_sample, \"Test Audio Waveform\")\n",
    "        \n",
    "        # Plot spectrogram\n",
    "        plot_spectrogram(audio_sample, \"Test Audio Spectrogram\")\n",
    "        \n",
    "        # Create audio widget for playback\n",
    "        print(\"\\nüîä Audio Playback:\")\n",
    "        display(Audio(audio_sample.audio_data, rate=audio_sample.sample_rate))\n",
    "        \n",
    "        print(\"‚úÖ Visualization demo complete!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Visualization demo failed: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå Test audio file not found\")\n",
    "\n",
    "print(\"\\n=== Format Support Test ===\")\n",
    "\n",
    "# Test different audio formats (create WAV and test loading)\n",
    "format_test_file = test_dir / \"format_test.wav\"\n",
    "if format_test_file.exists():\n",
    "    try:\n",
    "        # Test loading without preprocessing\n",
    "        raw_audio, raw_sr = audio_preprocessor.load_audio(str(format_test_file))\n",
    "        print(f\"‚úÖ Raw loading: {len(raw_audio)} samples at {raw_sr} Hz\")\n",
    "        \n",
    "        # Test sample rate normalization\n",
    "        normalized = audio_preprocessor.normalize_sample_rate(raw_audio, raw_sr)\n",
    "        print(f\"‚úÖ Normalized: {len(normalized)} samples at 16000 Hz\")\n",
    "        \n",
    "        # Test duration validation\n",
    "        duration = audio_preprocessor.validate_duration(normalized, 16000)\n",
    "        print(f\"‚úÖ Duration validation: {duration:.3f} seconds\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Format test failed: {e}\")\n",
    "\n",
    "print(\"\\nüéâ All preprocessing tests completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}